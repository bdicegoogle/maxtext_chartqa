# Gemma 3-12B Fine-tuning on ChartQA with MaxText

End-to-end pipeline for fine-tuning Google's Gemma 3-12B model on the ChartQA dataset using MaxText and Google Cloud TPUs.

## ğŸš€ Quick Start

```bash
# 1. Copy and configure environment
cp .env.example .env
# Edit .env with your configuration

# 2. Run the complete pipeline
./run_experiment.sh
```

## ğŸ“‹ Prerequisites

- **Google Cloud Project** with billing enabled and TPU quota
- **gcloud CLI** installed and authenticated
- **Docker** installed and configured
- **HuggingFace Token** with access to Gemma models
- **Python 3.12+** with `uv` package manager

## ğŸ”§ Setup

1. **Install dependencies:**
   ```bash
   uv sync
   ```

2. **Configure Google Cloud:**
   ```bash
   gcloud auth login
   gcloud auth configure-docker
   ```

3. **Configure environment:**
   ```bash
   cp .env.example .env
   # Edit .env with your project settings and HF token
   ```

## ğŸ—ï¸ Pipeline Overview

The `run_experiment.sh` script performs these steps:

1. **ğŸ› ï¸ Infrastructure Setup** - Enable APIs, configure permissions, create buckets
2. **ğŸ”„ Model Conversion** - Convert HuggingFace Gemma to MaxText format
3. **âœ… Model Validation** - Test converted model with sample inference
4. **ğŸ¯ Fine-tuning** - SFT on ChartQA dataset using TPUs
5. **ğŸ” Model Testing** - Validate fine-tuned model performance
6. **ğŸš€ Serving Deploy** - Deploy with JetStream for high-performance inference
7. **ğŸ“¦ Export** - Convert back to HuggingFace format

## ğŸ“Š Key Features

- **TPU Acceleration** - Optimized for Google Cloud TPU v5p/v6e
- **Multimodal Support** - Vision-language capabilities for chart understanding
- **Production Ready** - JetStream serving for high-throughput inference
- **Interactive Monitoring** - TensorBoard integration and step-by-step logging

## ğŸ›ï¸ Configuration

Key settings in `.env`:

| Variable | Description | Example |
|----------|-------------|---------|
| `TPU_TYPE` | TPU hardware type | `v5p-8`, `v6e-8` |
| `SFT_STEPS` | Fine-tuning steps | `250`, `1000` |
| `HF_TOKEN` | HuggingFace access token | `hf_xxx...` |
| `CLUSTER_NAME` | GKE cluster name | `gemma-experiment` |

## ğŸ“ˆ Monitoring

- **Training Progress:** `tensorboard --logdir=gs://your-bucket/path/tensorboard/`
- **Pod Status:** `kubectl get pods | grep gemma`
- **Logs:** `kubectl logs <pod-name> -f`

## ğŸ’° Cost Considerations

- **TPU Usage:** pricing is available at : https://cloud.google.com/tpu/pricing?hl=en 
- **Storage:** Minimal costs for model checkpoints
- **Duration:** 2-4 hours for complete pipeline

> âš ï¸ **Important:** Monitor costs in Google Cloud Console and delete resources when done.

## ğŸ” Expected Performance

- **Fine-tuning Speed:** ~250 steps in 30-60 minutes on v5p-8
- **Serving Throughput:** ~20-25 QPS with JetStream
- **Model Quality:** Enhanced chart reasoning capabilities

## ğŸ›Ÿ Troubleshooting

| Issue | Solution |
|-------|----------|
| TPU quota exceeded | Request quota increase or change region |
| HF authentication | Verify token has Gemma model access |
| Docker push fails | Run `gcloud auth configure-docker` |
| Pod pending | Check node availability and resource requests |

## ğŸ“ Project Structure

```
maxtext_chartqa/
â”œâ”€â”€ run_experiment.sh    # Main pipeline script
â”œâ”€â”€ .env.example        # Configuration template
â”œâ”€â”€ .env               # Your configuration (create from .env.example)
â”œâ”€â”€ inference/         # JetStream serving container
â”œâ”€â”€ pyproject.toml     # Python dependencies
â””â”€â”€ README.md          # This file
```

## ğŸ”— Resources

- [MaxText Documentation](https://github.com/AI-Hypercomputer/maxtext)
- [ChartQA Dataset](https://huggingface.co/datasets/HuggingFaceM4/ChartQA)
- [XPK User Guide](https://github.com/google/xpk)
- [Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)

## ğŸ“„ License

Apache 2.0 - See pipeline scripts for full license text.

---

*Built with â¤ï¸ for high-performance multimodal AI on Google Cloud TPUs*
