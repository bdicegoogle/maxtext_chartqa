# Copyright 2024 Google, LLC. This software is provided as-is,
# without warranty or representation for any use or purpose. Your
# use of it is subject to your agreement with Google.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# =============================================================================
# ENVIRONMENT CONFIGURATION TEMPLATE
# =============================================================================
# 
# This is a template file for configuring the Gemma 3-12B fine-tuning pipeline.
# Copy this file to .env and replace the placeholder values with your actual
# configuration.
# 
# USAGE:
#   cp .env.example .env
#   # Edit .env with your actual values
#   # Run: ./run_experiment.sh
#

# =============================================================================
# PROJECT CONFIGURATION
# =============================================================================
# These values are automatically retrieved from your gcloud configuration
PROJECT_ID=$(gcloud config get-value project)
USER=$(whoami)
GOOGLE_CLOUD_PROJECT=$(gcloud config get-value project)
PROJECT_NUMBER=$(gcloud projects describe ${PROJECT_ID} --format='value(projectNumber)')

# =============================================================================
# SERVICE ACCOUNTS
# =============================================================================
# These are constructed automatically from your project number
COMPUTE_SA=${PROJECT_NUMBER}-compute@developer.gserviceaccount.com
BUILD_SA=${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com

# =============================================================================
# CLUSTER CONFIGURATION
# =============================================================================
# Configure your GKE cluster and TPU settings
CLUSTER_NAME=<your-cluster-name>                    # Example: gemma-experiment
ZONE=<your-preferred-zone>                          # Example: us-central1-a, us-central2-b
REGION=<your-preferred-region>                      # Example: us-central1, us-central2
TPU_TYPE=<tpu-type>                                 # Example: v5p-8, v5litepod-16, v6e-8
NUM_SLICES=1                                        # Number of TPU slices to use

# =============================================================================
# STORAGE CONFIGURATION  
# =============================================================================
# GCS bucket names (will be created automatically)
OUTPUT_BUCKET=<your-project-id>-output-bucket       # Bucket for model outputs and logs
DATASET_BUCKET=<your-project-id>-dataset-bucket     # Bucket for training datasets
BASE_OUTPUT_DIR=gs://${OUTPUT_BUCKET}/
DATASET_PATH=gs://${DATASET_BUCKET}/datasets/

# =============================================================================
# DOCKER CONFIGURATION
# =============================================================================
# Docker registry and image names
DOCKER_REGISTRY=gcr.io/${PROJECT_ID}                # Or use artifact registry: us-docker.pkg.dev/${PROJECT_ID}
BASE_IMAGE=${DOCKER_REGISTRY}/<base-image-name>:latest     # Example: moonshot-saturn:latest
RUNNER_IMAGE=${DOCKER_REGISTRY}/<base-image-name>:latest   # Usually same as BASE_IMAGE
PLUTO_IMAGE=${DOCKER_REGISTRY}/<serving-image-name>:latest # Example: moonshot-pluto:latest

# =============================================================================
# KUBERNETES TOOLS VERSIONS
# =============================================================================
# Specific versions for reproducible builds
KJOB_VERSION=v0.1.0
KUEUE_VERSION=v0.13.2

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================
# Experiment and model settings
idx="<experiment-date-id>"                          # Example: 2025-08-20, experiment-v1
MODEL_NAME="<model-name>"                           # Example: gemma3-12b, gemma3-2b
MODEL_VARIATION="<model-size>"                      # Example: 12b, 2b, 8b
HF_TOKEN="<huggingface-access-token>"               # Get from https://huggingface.co/settings/tokens
HF_GOLDEN_MODEL="<huggingface-model-path>"          # Example: google/gemma-3-12b-it
TOKENIZER_PATH="<tokenizer-asset-path>"             # Example: assets/tokenizer.gemma3
USE_MULTIMODAL="true"                               # Enable multimodal capabilities: true/false
SCAN_LAYERS="false"                                 # Scan model layers: true/false  
SFT_STEPS="<number-of-training-steps>"              # Example: 250, 1000, 10000

# =============================================================================
# MODEL PATHS
# =============================================================================
# These paths are constructed automatically from the above configuration
MODEL_BUCKET="gs://${OUTPUT_BUCKET}/<model-name>"   # Will be: gs://your-bucket/gemma3-12b
UNSCANNED_CKPT_PATH=${MODEL_BUCKET}/${MODEL_VARIATION}/unscanned/${idx}/0/items
BASE_OUTPUT_DIRECTORY=${MODEL_BUCKET}/${MODEL_VARIATION}/unscanned/<experiment-type>  # Example: chartqa
FINAL_CKPT_STEP=$((SFT_STEPS - 1))
FINETUNED_CKPT_PATH=${BASE_OUTPUT_DIRECTORY}/${idx}/checkpoints/${FINAL_CKPT_STEP}/items
LOCAL_PATH=${MODEL_BUCKET}-<output-suffix>          # Example: ${MODEL_BUCKET}-chartqa

# =============================================================================
# SERVING CONFIGURATION
# =============================================================================
# Model serving parameters for JetStream
MAX_PREFILL_PREDICT_LENGTH=1024                     # Maximum prefill sequence length
MAX_TARGET_LENGTH=2048                              # Maximum generation length
ICI_FSDP_PARALLELISM=1                              # FSDP parallelism degree
ICI_AUTOREGRESSIVE_PARALLELISM=1                    # Autoregressive parallelism degree  
ICI_TENSOR_PARALLELISM=-1                           # Tensor parallelism (-1 for auto)
WEIGHT_DTYPE=bfloat16                               # Weight precision: bfloat16, float32
PER_DEVICE_BATCH_SIZE=16                            # Batch size per TPU device

# =============================================================================
# SETUP INSTRUCTIONS
# =============================================================================
# 
# 1. PREREQUISITES:
#    - Google Cloud Project with billing enabled
#    - gcloud CLI installed and authenticated (`gcloud auth login`)
#    - Docker installed and authenticated (`gcloud auth configure-docker`)
#    - Sufficient TPU quota in your selected region/zone
#    - HuggingFace account and access token for Gemma models
#
# 2. CONFIGURATION:
#    - Copy this file: `cp .env.example .env`
#    - Replace all <placeholder-values> with your actual configuration
#    - Ensure your HF_TOKEN has access to the Gemma model family
#    - Verify TPU availability in your chosen zone
#
# 3. EXECUTION:
#    - Make script executable: `chmod +x run_experiment.sh`
#    - Run the pipeline: `./run_experiment.sh`
#
# 4. MONITORING:
#    - Use TensorBoard for training metrics
#    - Monitor costs in Google Cloud Console
#    - Watch TPU utilization in GKE monitoring
#
# 5. IMPORTANT NOTES:
#    - TPU training incurs significant costs - monitor usage carefully
#    - The pipeline requires several hours to complete end-to-end
#    - Ensure sufficient disk space and memory for model conversion
#    - Keep your HuggingFace token secure and do not commit it to version control
#
